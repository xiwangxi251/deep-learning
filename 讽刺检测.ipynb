{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "04b9b943",
   "metadata": {},
   "source": [
    "# 作业五：实战NLP——讽刺检测\n",
    "\n",
    "\n",
    "| 姓名  |     学号     |\n",
    "|:---:|:----------:|\n",
    "| 艾华喜 | 1120222907 |\n",
    "\n",
    "### 要求：\n",
    "\n",
    "<font color=Red>完成以下notebook，Sarcasm Detection数据集的下载代码已经给出，请同学们自行完成数据处理和训练过程。作业提交 jupyter notebook 文件。</font>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5c326bcd",
   "metadata": {},
   "source": [
    "近年来，以社交媒体为媒介的电子新闻已成为信息消费的主要来源之一。许多媒体公司正在使用创造性的方法来增加帖子的浏览量。其中一种方法是使用讽刺标题作为用户点击的诱饵。\n",
    "\n",
    "一个能够预测一篇新闻的标题是否具有讽刺意味的模型对于媒体公司来说很有用，可以方便他们通过一些策略分析季度收益。此外，从读者的角度来看，搜索引擎可以利用这些讽刺的信息，并根据读者的偏好，向他们推荐类似的文章。\n",
    "\n",
    "## 数据集\n",
    "用于讽刺检测的新闻标题数据集，该数据集来自两个新闻网站，theonion.com和huffingtonpost.com。以往的研究大多使用基于标签监督收集的Twitter数据集，但这些数据集在标签和语言方面存在噪声。此外，许多tweet是对其他tweet的回复，检测其中的讽刺需要上下文tweet的信息。这个新的数据集与现有的Twitter数据集相比有以下优点:\n",
    "由于新闻标题是由专业人士以正式的方式编写的，所以没有拼写错误和非正式用法。这减少了稀疏性。\n",
    "此外，由于TheOnion的唯一目的是发布讽刺的新闻，与Twitter数据集相比，标签的质量要更高，噪音小得多。与回复其他推文的推文不同，新闻标题是独立的。这将有助于我们梳理出真正的讽刺元素"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c57d452d",
   "metadata": {},
   "source": [
    "## 下载和缓存数据集"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "d9c94b00",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting wget\n",
      "  Downloading wget-3.2.zip (10 kB)\n",
      "  Preparing metadata (setup.py): started\n",
      "  Preparing metadata (setup.py): finished with status 'done'\n",
      "Building wheels for collected packages: wget\n",
      "  Building wheel for wget (setup.py): started\n",
      "  Building wheel for wget (setup.py): finished with status 'done'\n",
      "  Created wheel for wget: filename=wget-3.2-py3-none-any.whl size=9657 sha256=f3333ddb6fc5567524467c4aee9caace920dc65dd3a2647945311682924e40ed\n",
      "  Stored in directory: c:\\users\\oliver\\appdata\\local\\pip\\cache\\wheels\\ba\\78\\fb\\e0c24a9e73d7483b073d15b7e05f43f3fc2ac75eff6899c7aa\n",
      "Successfully built wget\n",
      "Installing collected packages: wget\n",
      "Successfully installed wget-3.2\n"
     ]
    }
   ],
   "source": "!pip install wget"
  },
  {
   "cell_type": "code",
   "id": "5ae50ce7",
   "metadata": {},
   "source": [
    "import wget\n",
    "import os\n",
    "\n",
    "target_dir = '../data/'\n",
    "target_filename = 'Sarcasm_News_Headline.json'\n",
    "\n",
    "if not os.path.exists(target_dir):\n",
    "    os.makedirs(target_dir)\n",
    "\n",
    "url = 'https://huggingface.co/datasets/raquiba/Sarcasm_News_Headline/resolve/main/test.json?download=true'\n",
    "downloaded_file_path = wget.download(url, target_dir)\n",
    "\n",
    "new_file_path = os.path.join(target_dir, target_filename)\n",
    "os.rename(downloaded_file_path, new_file_path)"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "id": "3686978f",
   "metadata": {},
   "source": [
    "## 读取并查看数据集"
   ]
  },
  {
   "cell_type": "code",
   "id": "8a7eceb3",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-06-28T12:07:52.669536Z",
     "start_time": "2024-06-28T12:07:52.591552Z"
    }
   },
   "source": [
    "import json\n",
    "\n",
    "data_raw = [json.loads(line) for \n",
    "        line in open(new_file_path, 'r')]"
   ],
   "outputs": [],
   "execution_count": 3
  },
  {
   "cell_type": "code",
   "id": "c247cd92",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-06-28T12:07:55.778741Z",
     "start_time": "2024-06-28T12:07:55.775837Z"
    }
   },
   "source": [
    "print(len(data_raw))\n",
    "print(data_raw[0])"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "26709\n",
      "{'article_link': 'https://www.huffingtonpost.com/entry/versace-black-code_us_5861fbefe4b0de3a08f600d5', 'headline': \"former versace store clerk sues over secret 'black code' for minority shoppers\", 'is_sarcastic': 0}\n"
     ]
    }
   ],
   "execution_count": 4
  },
  {
   "cell_type": "markdown",
   "id": "fc2357c6",
   "metadata": {},
   "source": [
    "可以看到数据集一共包含了26709条新闻标题以及对应的标签。**这里先忽略数据集里的'article_link'属性**\n",
    "\n",
    "数据集的 **'headline'** 给出的是新闻标题，而 **'is_sarcastic'** 给出的是该新闻标题是否是讽刺性的标签。\n",
    "\n",
    "下面看一下数据集里所有 **'headline'** 的长度统计数据。"
   ]
  },
  {
   "cell_type": "code",
   "id": "80ab22be",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-06-28T12:08:51.633567Z",
     "start_time": "2024-06-28T12:08:51.610986Z"
    }
   },
   "source": [
    "max_length, min_length = 0, 0x3f3f3f\n",
    "sum_length = 0\n",
    "length_distribute = [0] * 1000\n",
    "for i in range(len(data_raw)):\n",
    "    l = len(data_raw[i]['headline'])\n",
    "    sum_length += l\n",
    "    max_length = max(max_length, l)\n",
    "    min_length = min(min_length, l)\n",
    "    length_distribute[l] += 1\n",
    "        \n",
    "avg = sum_length / len(data_raw)\n",
    "print(f'max length: {max_length} \\nmin length: {min_length} \\navg length: {avg}')\n",
    "\n",
    "print(length_distribute[:max_length + 1])"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "max length: 254 \n",
      "min length: 7 \n",
      "avg length: 60.910591935302705\n",
      "[0, 0, 0, 0, 0, 0, 0, 1, 4, 4, 2, 13, 10, 19, 38, 30, 33, 43, 52, 49, 63, 68, 74, 69, 91, 117, 93, 131, 152, 146, 164, 170, 202, 210, 216, 255, 228, 235, 250, 315, 331, 316, 371, 356, 364, 370, 420, 403, 435, 436, 475, 497, 488, 460, 490, 502, 553, 520, 543, 530, 550, 577, 645, 581, 600, 673, 575, 575, 532, 570, 532, 529, 457, 497, 431, 439, 423, 387, 353, 338, 312, 254, 292, 256, 250, 213, 210, 194, 185, 155, 146, 141, 122, 113, 106, 89, 91, 71, 67, 66, 67, 59, 50, 49, 43, 33, 39, 33, 21, 22, 29, 24, 23, 31, 12, 16, 17, 15, 8, 13, 7, 8, 7, 9, 9, 6, 3, 7, 5, 2, 2, 5, 1, 2, 1, 0, 1, 4, 1, 3, 0, 2, 1, 4, 0, 2, 0, 0, 0, 1, 0, 0, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1]\n"
     ]
    }
   ],
   "execution_count": 5
  },
  {
   "cell_type": "markdown",
   "id": "8a925aad",
   "metadata": {},
   "source": [
    "可以看到最长的标题到达了254个单词之多，而最短只有7个单词。平均长度为60。"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8d80790d",
   "metadata": {},
   "source": [
    "# <font color=Red>数据处理（自行完成）<font>\n",
    "\n",
    "\n",
    "\n",
    "请同学自己完成数据处理过程。\n",
    "    \n",
    "<font color=Red> 要求： <font> \n",
    "    \n",
    "**26709条新闻的前20000个作为训练集，后6709条作为测试集，不设验证集。** 最后在测试集上测试自己模型的最终结果。"
   ]
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "import json\n",
    "import os\n",
    "import torch\n",
    "from transformers import BertTokenizer, BertModel\n",
    "from torch.optim import AdamW\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import numpy as np\n",
    "import warnings\n",
    "\n",
    "# 忽略警告\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# 目标目录和文件名\n",
    "target_dir = '../data/'\n",
    "target_filename = 'Sarcasm_News_Headline.json'\n",
    "\n",
    "# 定义用于讽刺检测的Dataset类\n",
    "class SarcasmDataset(Dataset):\n",
    "    def __init__(self, sentences, labels, tokenizer, max_length):\n",
    "        self.sentences = sentences  # 保存句子\n",
    "        self.labels = labels  # 保存标签\n",
    "        self.tokenizer = tokenizer  # 保存分词器\n",
    "        self.max_length = max_length  # 保存最大长度\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.sentences)  # 返回数据集的长度\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        # 获取句子和标签\n",
    "        sentence = self.sentences[idx]\n",
    "        label = self.labels[idx]\n",
    "\n",
    "        # 编码句子\n",
    "        encoding = self.tokenizer.encode_plus(\n",
    "            sentence,\n",
    "            add_special_tokens=True,\n",
    "            max_length=self.max_length,\n",
    "            padding='max_length',\n",
    "            return_attention_mask=True,\n",
    "            return_tensors='pt',\n",
    "            truncation=True\n",
    "        )\n",
    "\n",
    "        return {\n",
    "            'input_ids': encoding['input_ids'].flatten(),  # 返回输入ID\n",
    "            'attention_mask': encoding['attention_mask'].flatten(),  # 返回注意力掩码\n",
    "            'labels': torch.tensor(label, dtype=torch.long)  # 返回标签\n",
    "        }\n",
    "\n",
    "# 加载数据集\n",
    "def load_dataset(file_path):\n",
    "    # 读取文件并解析为JSON对象\n",
    "    data_raw = [json.loads(line) for line in open(file_path, 'r')]\n",
    "    # 提取句子和标签\n",
    "    sentences = [item['headline'] for item in data_raw]\n",
    "    labels = [item['is_sarcastic'] for item in data_raw]\n",
    "    return sentences, labels\n",
    "\n",
    "# 编码单个句子\n",
    "def encode_sentence(sentence):\n",
    "    inputs = tokenizer.encode_plus(\n",
    "        sentence,\n",
    "        add_special_tokens=True,\n",
    "        max_length=254,\n",
    "        padding='max_length',\n",
    "        return_attention_mask=True,\n",
    "        return_tensors='pt'\n",
    "    )\n",
    "    return inputs['input_ids'], inputs['attention_mask']\n",
    "\n",
    "# 处理数据，将句子编码为BERT嵌入向量\n",
    "def process_data(sentences, labels, model):\n",
    "    x = []  # 保存句子嵌入\n",
    "    for i, sentence in enumerate(sentences):\n",
    "        print(i)\n",
    "        input_ids, attention_mask = encode_sentence(sentence)\n",
    "        with torch.no_grad():  # 禁用梯度计算\n",
    "            outputs = model(input_ids, attention_mask=attention_mask)\n",
    "        # 获取句子的平均嵌入\n",
    "        sentence_embedding = outputs.last_hidden_state.mean(dim=1).squeeze()\n",
    "        x.append(sentence_embedding)\n",
    "    data = {\n",
    "        'X': x,  # 保存句子嵌入\n",
    "        'Y': labels  # 保存标签\n",
    "    }\n",
    "    np.save(\"train_data.npy\", data)  # 保存为.npy文件\n",
    "\n",
    "# 预训练BERT模型名称\n",
    "model_name = '../bert-base-uncased'\n",
    "\n",
    "# 加载训练数据集\n",
    "train_sentences, train_labels = load_dataset(os.path.join(target_dir, target_filename))\n",
    "\n",
    "# 加载BERT分词器\n",
    "tokenizer = BertTokenizer.from_pretrained(model_name)\n",
    "\n",
    "# 创建数据集和数据加载器\n",
    "train_dataset = SarcasmDataset(train_sentences, train_labels, tokenizer, max_length=254)\n",
    "train_loader = DataLoader(train_dataset, batch_size=64, shuffle=True)\n",
    "\n",
    "# 加载BERT模型\n",
    "model = BertModel.from_pretrained(model_name)\n",
    "\n",
    "# 处理数据，生成句子嵌入\n",
    "process_data(train_sentences, train_labels, model)\n"
   ],
   "id": "3e4b95f1bd855512"
  },
  {
   "cell_type": "markdown",
   "id": "840772e8",
   "metadata": {},
   "source": [
    "# <font color=Red>训练（自行完成）<font>\n",
    "\n",
    "请同学自己完成从**定义网络、定义损失函数、定义优化器到进行训练等一系列深度学习流水线**。"
   ]
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-06-29T10:48:26.837431Z",
     "start_time": "2024-06-29T10:48:18.827753Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import random\n",
    "import torch.nn.functional as F\n",
    "import torch\n",
    "from torch import nn\n",
    "import numpy as np\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "def date_iter(batch_size,X,Y):\n",
    "    num_examples = len(X)\n",
    "    indices = list(range(num_examples))\n",
    "    random.shuffle(indices)\n",
    "    for i in range(0, num_examples, batch_size):\n",
    "        batch_indices = torch.tensor(\n",
    "            indices[i: min(i + batch_size, num_examples)])\n",
    "        yield X[batch_indices], Y[batch_indices]\n",
    "\n",
    "\n",
    "class LogisticRegression(nn.Module):\n",
    "    def __init__(self, input_size, hidden_size, output_size):\n",
    "        super(LogisticRegression, self).__init__()\n",
    "        self.fc1 = nn.Linear(input_size, hidden_size)\n",
    "        self.fc2 = nn.Linear(hidden_size, output_size)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = F.relu(self.fc1(x))\n",
    "        x = self.fc2(x)\n",
    "        return torch.sigmoid(x)\n",
    "\n",
    "def train(num_epochs):\n",
    "    loaded_data = np.load('train_data.npy', allow_pickle=True)\n",
    "    trainX = loaded_data.item().get('X')\n",
    "    trainY = loaded_data.item().get('Y')\n",
    "    train_X = torch.stack(trainX)[:20000]\n",
    "    train_Y = torch.tensor(trainY)[:20000]\n",
    "\n",
    "    for epoch in range(num_epochs):\n",
    "        batch_count = 0  # 初始化batch计数器\n",
    "\n",
    "        total_loss = 0\n",
    "        for x, y in date_iter(batch_size, train_X, train_Y):\n",
    "            x,y=x.float(),y.float()\n",
    "            x,y = x.to(device), y.to(device)\n",
    "            y=y.unsqueeze(1)\n",
    "            batch_count += 1  # 每次迭代时增加计数\n",
    "            updater.zero_grad()\n",
    "            y_hat = logistic(x)\n",
    "            l = loss(y_hat, y)\n",
    "            total_loss += l.item()\n",
    "            l.backward()\n",
    "            updater.step()\n",
    "        print(f'epoch: {epoch}, loss: {total_loss / batch_count}' )\n",
    "    torch.save(logistic.state_dict(), 'model.pth')  # 保存模型的参数\n",
    "\n",
    "\n",
    "input_size = 768  # 输入向量的长度\n",
    "hidden_size = 256 # 隐藏层的大小\n",
    "output_size = 1  # 输出大小，二分类问题只需输出一个值\n",
    "logistic = LogisticRegression(input_size, hidden_size, output_size)\n",
    "batch_size = 256\n",
    "num_epochs = 40\n",
    "lr = 0.01\n",
    "loss = nn.BCELoss()\n",
    "\n",
    "updater = torch.optim.Adam(logistic.parameters(), lr=lr)\n",
    "device='cuda' if torch.cuda.is_available() else 'cpu'\n",
    "logistic.to(device)\n",
    "\n",
    "\n",
    "train(num_epochs)\n",
    "\n"
   ],
   "id": "19592172cf770ea0",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 0, loss: 0.4133866063401669\n",
      "epoch: 1, loss: 0.3038702858022497\n",
      "epoch: 2, loss: 0.27173037317734733\n",
      "epoch: 3, loss: 0.2500445742773104\n",
      "epoch: 4, loss: 0.233459790673437\n",
      "epoch: 5, loss: 0.2215737034625645\n",
      "epoch: 6, loss: 0.20715269484097446\n",
      "epoch: 7, loss: 0.19154358890992176\n",
      "epoch: 8, loss: 0.17954917195477063\n",
      "epoch: 9, loss: 0.17692906334053113\n",
      "epoch: 10, loss: 0.15234877291736723\n",
      "epoch: 11, loss: 0.1409556182879436\n",
      "epoch: 12, loss: 0.131787016610556\n",
      "epoch: 13, loss: 0.11256198962278004\n",
      "epoch: 14, loss: 0.09802745565583434\n",
      "epoch: 15, loss: 0.08877636001834387\n",
      "epoch: 16, loss: 0.08167257617357411\n",
      "epoch: 17, loss: 0.06800768175457098\n",
      "epoch: 18, loss: 0.06177805189656306\n",
      "epoch: 19, loss: 0.0658184605403037\n",
      "epoch: 20, loss: 0.049643897581138186\n",
      "epoch: 21, loss: 0.03725508263311054\n",
      "epoch: 22, loss: 0.03403293209362634\n",
      "epoch: 23, loss: 0.028151061397658873\n",
      "epoch: 24, loss: 0.023568490803053108\n",
      "epoch: 25, loss: 0.02060372084143418\n",
      "epoch: 26, loss: 0.025778521406405335\n",
      "epoch: 27, loss: 0.015356962292018947\n",
      "epoch: 28, loss: 0.01068233950299364\n",
      "epoch: 29, loss: 0.007858168466393894\n",
      "epoch: 30, loss: 0.005399959705640338\n",
      "epoch: 31, loss: 0.0037654896747155845\n",
      "epoch: 32, loss: 0.0032585106975740836\n",
      "epoch: 33, loss: 0.002443088136047502\n",
      "epoch: 34, loss: 0.002109185876501606\n",
      "epoch: 35, loss: 0.0018380807677307461\n",
      "epoch: 36, loss: 0.0017457173649469227\n",
      "epoch: 37, loss: 0.0015215347852569686\n",
      "epoch: 38, loss: 0.0013400119341780209\n",
      "epoch: 39, loss: 0.0013125466250614086\n"
     ]
    }
   ],
   "execution_count": 2
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "# <font color=Red>测试<font>\n",
   "id": "b6a14b515dc825ee"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "",
   "id": "8782a923590730cf"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-06-29T10:49:45.488236Z",
     "start_time": "2024-06-29T10:49:43.126135Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import random\n",
    "import torch.nn.functional as F\n",
    "import torch\n",
    "from torch import nn\n",
    "import numpy as np\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "def date_iter(batch_size,X,Y):\n",
    "    num_examples = len(X)\n",
    "    indices = list(range(num_examples))\n",
    "    for i in range(0, num_examples, batch_size):\n",
    "        batch_indices = torch.tensor(\n",
    "            indices[i: min(i + batch_size, num_examples)])\n",
    "        yield X[batch_indices], Y[batch_indices]\n",
    "\n",
    "class Accumulator:\n",
    "    \"\"\"用于累加 `n` 个变量的累加器。\"\"\"\n",
    "    def __init__(self, n):\n",
    "        self.data = [0.0] * n\n",
    "\n",
    "    def add(self, *args):\n",
    "        self.data = [a + float(b) for a, b in zip(self.data, args)]\n",
    "\n",
    "    def reset(self):\n",
    "        self.data = [0.0] * len(self.data)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        return self.data[idx]\n",
    "class LogisticRegression(nn.Module):\n",
    "    def __init__(self, input_size, hidden_size, output_size):\n",
    "        super(LogisticRegression, self).__init__()\n",
    "        self.fc1 = nn.Linear(input_size, hidden_size)\n",
    "        self.fc2 = nn.Linear(hidden_size, output_size)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = F.relu(self.fc1(x))\n",
    "        x = self.fc2(x)\n",
    "        return torch.sigmoid(x)\n",
    "\n",
    "def test():\n",
    "    loaded_data = np.load('train_data.npy', allow_pickle=True)\n",
    "    trainX = loaded_data.item().get('X')\n",
    "    trainY = loaded_data.item().get('Y')\n",
    "    train_X = torch.stack(trainX)[20000:]\n",
    "    train_Y = torch.tensor(trainY)[20000:]\n",
    "    metric = Accumulator(2)\n",
    "\n",
    "\n",
    "    for x, y in date_iter(batch_size, train_X, train_Y):\n",
    "\n",
    "            x,y=x.float(),y.float()\n",
    "            x,y = x.to(device), y.to(device)\n",
    "            y=y.unsqueeze(1)\n",
    "            y_hat = logistic(x)\n",
    "            y_hat = (y_hat > 0.5).float()\n",
    "            same_elements = torch.sum(y == y_hat).item()\n",
    "            metric.add(same_elements, y.size(0))\n",
    "\n",
    "    print(f'accuracy{metric[0] / metric[1]:.10f}' )\n",
    "\n",
    "\n",
    "input_size = 768  # 输入向量的长度\n",
    "hidden_size = 256 # 隐藏层的大小\n",
    "output_size = 1  # 输出大小，二分类问题只需输出一个值\n",
    "logistic = LogisticRegression(input_size, hidden_size, output_size)\n",
    "logistic.load_state_dict(torch.load('model.pth'))\n",
    "batch_size = 256\n",
    "num_epochs = 40\n",
    "lr = 0.01\n",
    "loss = nn.BCELoss()\n",
    "\n",
    "device='cuda' if torch.cuda.is_available() else 'cpu'\n",
    "logistic.to(device)\n",
    "\n",
    "\n",
    "test()\n",
    "\n"
   ],
   "id": "86a032a18a0f3ecd",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "accuracy0.8806081383\n"
     ]
    }
   ],
   "execution_count": 3
  },
  {
   "cell_type": "markdown",
   "id": "8fbe0d7a",
   "metadata": {},
   "source": [
    "## 提交方式\n",
    "<font color=Red>包含训练结果的Jupyter notebook文件请命名为 `work2_<组长姓名>_<组长学号>.ipynb` 发送到邮箱 archie98@qq.com</font>"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "deeplr",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
